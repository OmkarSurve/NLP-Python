
##### The below function is used to summarize the article from web page
##### Here in this case, using this function we can take any article from washington post website and summarize the article in 
##### n lines


#### Basic function for any text related activity is to tokenize the sentences and words, and removal of stopwords

from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import stopwords

#### defaultdict is special dictionary with all functions of mormal dictionary, except it insert a default value in 
#### case we need to use new entry which already dose not exist in the dictionary

from collections import defaultdict

### we have to check whether the symbol is a punctuation symbol or not
from string import punctuation

### function to get n largest elements ftom the list
from heapq import nlargest

### Following class will have all methods from tokenization, removing stop words
### Count of words, ranking of sentences 

class ArticleSummary:
    ### Constructor is present in all the classes
    ### It will instantiate everytime, an object is created for a class
    def __init__(self, min_thr=0.1, max_thr=0.9):
        self._min_thr = min_thr
        self._max_thr = max_thr 
        
        ### variables starting with self are member variables, self is used to differentiate member varible from 
        ### normal variables
        self._stopwords = set(stopwords.words('english') + list(punctuation))
        
        #### if we define, member variables outside the class, and not in a member function then unlike java it becomes 
        #### static, i.e. it becomes available to the class and not to any individual instance (object) of the class
        #### and it should be avoided
        
    
    #### This function will compute the frequency of each word in a piece of text
    #### In end it will return a dictionary with word and its frequencies
    

    def _word_frequency(self, sent_list):
        ### word_sent is the list of sentences
        word_freq = defaultdict(int)
        
        ### we define the dict as defaultdict
        
        ### we define for loop to loop through words in each sentence to get frequency of each word and
        ### update the dictionary
        
        for sent in sent_list:
             for word in sent:
                if word not in self._stopwords:
                    word_freq[word] += 1
        
        m = float(max(word_freq.values()))
        
        ### We will normalize the frequencies by dividing each frequency with highest frequency so as to get 
        ### value between 0 and 1
        
        for w in list(word_freq.keys()):
            word_freq[w] = word_freq[w]/m
        
        #### we will also filter out words which appear too often or too little
            if word_freq[w] >= self._max_thr or word_freq[w] <= self._min_thr:
                del word_freq[w]
        
        return word_freq
    
    

    
    ### Now we will rank the sentence as per their importance in text, based on the frequency of word in the sentence
    
    def rank(self, text, n):
        ### text : text to summarize
        ### n : no of sentences in which we want to summarize the text 
        
        sents_list = sent_tokenize(text)
        assert n <= len(sents_list)
        
        
        #assert n <= len(sents)
        ### Checks n should be less that actual sentences in text
        words = [word_tokenize(s.lower()) for s in sents_list]
        
        self._freq_word = self._word_frequency(words)
        
        ranking = defaultdict(int)
        ### Define defaultdict dictionary to rank sentences
        
        for i,sents in enumerate(words):
            for w in sents:
                if w in self._freq_word:
                    ranking[i] += self._freq_word[w]
                    
        sents_idx = nlargest(n, ranking, key=ranking.get)
        ### return index of n most important sentences
        
        return [sents_list[j] for j in sents_idx]

#### BeautifulSoup is the library used to extract the text from html page

import urllib2
from bs4 import BeautifulSoup
    
    
### Function to get text from the url

def washington_post_url(url):
    article = urllib2.urlopen(url).read().decode('utf8')
    
    
    soup = BeautifulSoup(article)
    
    
    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))
    ### This will join together text in between article tags
    
    soup2 = BeautifulSoup(text)
    if soup2.find_all('p')!=[]:
    
        text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))
    ### This will join together text in between p tags
    
    return soup.title.text, text

### This will return the raw article text and title of the text


    
### Lets implement the function now

Url = "https://www.washingtonpost.com/news/the-intersect/wp/2017/01/24/with-worldstarhiphop-lee-odenat-pioneered-our-viral-video-addiction/?hpid=hp_hp-more-top-stories_worldstarhiphop%3Ahomepage%2Fstory&utm_term=.2d414e624f62"

extractText = washington_post_url(Url)


arsum = ArticleSummary()

summary = arsum.rank(extractText[1], 3)

print summary
